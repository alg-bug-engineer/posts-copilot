#!/usr/bin/env python3
"""
test_news_sources.py

æµ‹è¯•å¤šæ–°é—»æºåŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from generate.aibase_crawler import AIBaseCrawler
from generate.qbitai_crawler import QbitAICrawler


def test_crawler(crawler_name, crawler_class, limit=5):
    """æµ‹è¯•å•ä¸ªçˆ¬è™«"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª æµ‹è¯• {crawler_name} çˆ¬è™«")
    print(f"{'='*80}\n")
    
    try:
        crawler = crawler_class()
        news_list = crawler.fetch_top_news(limit=limit)
        
        if news_list:
            print(f"\nâœ… {crawler_name} æµ‹è¯•æˆåŠŸï¼")
            print(f"   æŠ“å–äº† {len(news_list)} æ¡æ–°é—»")
            print(f"\n   ç¤ºä¾‹æ–°é—»ï¼š")
            for idx, news in enumerate(news_list[:3], 1):
                print(f"   [{idx}] {news['title'][:60]}...")
                print(f"       é“¾æ¥: {news['url']}")
                print(f"       æ—¶é—´: {news.get('time', 'N/A')}")
        else:
            print(f"âŒ {crawler_name} æµ‹è¯•å¤±è´¥ï¼šæœªæŠ“å–åˆ°æ–°é—»")
            
    except Exception as e:
        print(f"âŒ {crawler_name} æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸš€ å¤šæ–°é—»æºåŠŸèƒ½æµ‹è¯•")
    print("="*80)
    
    # æµ‹è¯• AIBase çˆ¬è™«
    test_crawler("AIBase", AIBaseCrawler, limit=5)
    
    # æµ‹è¯•é‡å­ä½çˆ¬è™«
    test_crawler("é‡å­ä½", QbitAICrawler, limit=5)
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
