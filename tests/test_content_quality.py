#!/usr/bin/env python3
"""
test_content_quality.py

æµ‹è¯•ç”Ÿæˆå†…å®¹çš„è´¨é‡ï¼šé•¿åº¦ã€è‡ªç„¶åº¦ã€é£æ ¼
"""

import os
import sys
import re
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))


def analyze_content_quality(content: str) -> dict:
    """åˆ†æå†…å®¹è´¨é‡"""
    
    # ç§»é™¤Front Matter
    content_without_fm = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)
    
    # ç»Ÿè®¡æŒ‡æ ‡
    char_count = len(content_without_fm)
    line_count = len(content_without_fm.split('\n'))
    paragraphs = [p for p in content_without_fm.split('\n\n') if p.strip()]
    paragraph_count = len(paragraphs)
    
    # æ£€æµ‹AIç—•è¿¹è¯æ±‡
    ai_phrases = [
        'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æœ€å',
        'æ€»ä¹‹', 'ç»¼ä¸Šæ‰€è¿°', 'æ€»çš„æ¥è¯´',
        'å€¼å¾—æ³¨æ„çš„æ˜¯', 'éœ€è¦æŒ‡å‡ºçš„æ˜¯',
        'é€šè¿‡...å®ç°', 'åŸºäº...æŠ€æœ¯',
        'ä¸º...æä¾›äº†', 'ä½¿å¾—...æˆä¸ºå¯èƒ½'
    ]
    
    ai_phrase_count = 0
    found_phrases = []
    for phrase in ai_phrases:
        if phrase in content_without_fm:
            ai_phrase_count += 1
            found_phrases.append(phrase)
    
    # æ£€æµ‹ç« èŠ‚æ ‡é¢˜ï¼ˆ##ï¼‰
    section_titles = re.findall(r'^##\s+(.+)$', content_without_fm, re.MULTILINE)
    section_count = len(section_titles)
    
    # æ£€æµ‹å†’å·æ ‡é¢˜
    colon_titles = [t for t in section_titles if 'ï¼š' in t or ':' in t]
    
    # æ£€æµ‹æ€»ç»“æ®µè½
    summary_indicators = ['æ€»ç»“', 'ç»¼ä¸Š', 'æœ€åè¯´', 'å†™åœ¨æœ€å']
    has_summary = any(indicator in content_without_fm for indicator in summary_indicators)
    
    # æ®µè½é•¿åº¦åˆ†å¸ƒ
    paragraph_lengths = [len(p) for p in paragraphs]
    avg_paragraph_length = sum(paragraph_lengths) / len(paragraph_lengths) if paragraph_lengths else 0
    
    return {
        'char_count': char_count,
        'line_count': line_count,
        'paragraph_count': paragraph_count,
        'avg_paragraph_length': avg_paragraph_length,
        'ai_phrase_count': ai_phrase_count,
        'found_ai_phrases': found_phrases,
        'section_count': section_count,
        'section_titles': section_titles,
        'colon_titles': colon_titles,
        'has_summary': has_summary
    }


def grade_content(metrics: dict) -> str:
    """ç»™å†…å®¹æ‰“åˆ†"""
    score = 100
    issues = []
    
    # é•¿åº¦æ£€æŸ¥
    if metrics['char_count'] < 1500:
        score -= 30
        issues.append(f"âŒ å†…å®¹è¿‡çŸ­ ({metrics['char_count']}å­—ç¬¦ï¼Œå»ºè®®â‰¥2000)")
    elif metrics['char_count'] < 2000:
        score -= 10
        issues.append(f"âš ï¸ å†…å®¹åçŸ­ ({metrics['char_count']}å­—ç¬¦ï¼Œå»ºè®®â‰¥2000)")
    else:
        issues.append(f"âœ… å†…å®¹é•¿åº¦è¾¾æ ‡ ({metrics['char_count']}å­—ç¬¦)")
    
    # AIç—•è¿¹æ£€æŸ¥
    if metrics['ai_phrase_count'] > 5:
        score -= 20
        issues.append(f"âŒ AIç—•è¿¹ä¸¥é‡ (å‘ç°{metrics['ai_phrase_count']}å¤„)")
    elif metrics['ai_phrase_count'] > 2:
        score -= 10
        issues.append(f"âš ï¸ AIç—•è¿¹è¾ƒå¤š (å‘ç°{metrics['ai_phrase_count']}å¤„: {', '.join(metrics['found_ai_phrases'][:3])})")
    else:
        issues.append(f"âœ… è¯­è¨€è‡ªç„¶")
    
    # ç« èŠ‚æ ‡é¢˜æ£€æŸ¥
    if metrics['section_count'] > 5:
        score -= 10
        issues.append(f"âš ï¸ å­æ ‡é¢˜è¿‡å¤š ({metrics['section_count']}ä¸ª)")
    elif metrics['section_count'] == 0:
        issues.append(f"âœ… æµç•…è¡¨è¾¾ï¼Œæ— å¤šä½™ç« èŠ‚")
    else:
        issues.append(f"âœ“ ç« èŠ‚åˆ’åˆ†é€‚åº¦ ({metrics['section_count']}ä¸ª)")
    
    # å†’å·æ ‡é¢˜æ£€æŸ¥
    if metrics['colon_titles']:
        score -= 15
        issues.append(f"âŒ æ ‡é¢˜ä½¿ç”¨å†’å· ({len(metrics['colon_titles'])}å¤„)")
    else:
        issues.append(f"âœ… æ ‡é¢˜è¡¨è¾¾è‡ªç„¶")
    
    # æ€»ç»“æ£€æŸ¥
    if metrics['has_summary']:
        score -= 10
        issues.append(f"âš ï¸ æœ‰æ€»ç»“æ®µè½ï¼ˆä¸å¤Ÿçµæ´»ï¼‰")
    else:
        issues.append(f"âœ… ç»“å°¾è‡ªç„¶")
    
    # æ®µè½é•¿åº¦æ£€æŸ¥
    if metrics['avg_paragraph_length'] > 500:
        score -= 10
        issues.append(f"âš ï¸ æ®µè½åé•¿ (å¹³å‡{metrics['avg_paragraph_length']:.0f}å­—)")
    else:
        issues.append(f"âœ… æ®µè½é•¿åº¦é€‚ä¸­")
    
    # è¯„çº§
    if score >= 90:
        grade = "A (ä¼˜ç§€)"
    elif score >= 80:
        grade = "B (è‰¯å¥½)"
    elif score >= 70:
        grade = "C (åŠæ ¼)"
    else:
        grade = "D (éœ€æ”¹è¿›)"
    
    return grade, score, issues


def test_file(file_path: str):
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶"""
    print(f"\n{'='*80}")
    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {Path(file_path).name}")
    print(f"{'='*80}\n")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        metrics = analyze_content_quality(content)
        grade, score, issues = grade_content(metrics)
        
        print(f"ğŸ“Š å†…å®¹åˆ†æ")
        print(f"-" * 80)
        print(f"å­—ç¬¦æ•°: {metrics['char_count']}")
        print(f"æ®µè½æ•°: {metrics['paragraph_count']}")
        print(f"ç« èŠ‚æ•°: {metrics['section_count']}")
        print(f"AIç—•è¿¹: {metrics['ai_phrase_count']}å¤„")
        
        if metrics['section_titles']:
            print(f"\nç« èŠ‚æ ‡é¢˜:")
            for title in metrics['section_titles']:
                print(f"  - {title}")
        
        print(f"\nğŸ“ˆ è´¨é‡è¯„åˆ†")
        print(f"-" * 80)
        print(f"ç»¼åˆå¾—åˆ†: {score}/100")
        print(f"è´¨é‡è¯„çº§: {grade}")
        
        print(f"\nğŸ“ è¯¦ç»†è¯„ä»·")
        print(f"-" * 80)
        for issue in issues:
            print(f"  {issue}")
        
        return score
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return 0


def test_directory(directory: str):
    """æµ‹è¯•ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª æ‰¹é‡æµ‹è¯•å†…å®¹è´¨é‡")
    print(f"{'='*80}")
    
    posts_dir = Path(directory)
    if not posts_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return
    
    md_files = list(posts_dir.glob("*.md"))
    if not md_files:
        print(f"âŒ æœªæ‰¾åˆ°Markdownæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡ä»¶\n")
    
    scores = []
    for file_path in md_files:
        score = test_file(str(file_path))
        scores.append(score)
    
    if scores:
        avg_score = sum(scores) / len(scores)
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ€»ä½“è¯„ä»·")
        print(f"{'='*80}")
        print(f"å¹³å‡åˆ†æ•°: {avg_score:.1f}/100")
        print(f"æœ€é«˜åˆ†: {max(scores)}")
        print(f"æœ€ä½åˆ†: {min(scores)}")
        
        if avg_score >= 85:
            print(f"\nâœ… æ•´ä½“è´¨é‡ä¼˜ç§€ï¼")
        elif avg_score >= 75:
            print(f"\nâœ“ æ•´ä½“è´¨é‡è‰¯å¥½")
        else:
            print(f"\nâš ï¸ éœ€è¦æ”¹è¿›")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ–‡ç« å†…å®¹è´¨é‡')
    parser.add_argument(
        'path',
        nargs='?',
        help='æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„'
    )
    
    args = parser.parse_args()
    
    if args.path:
        path = Path(args.path)
        if path.is_file():
            test_file(str(path))
        elif path.is_dir():
            test_directory(str(path))
        else:
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {args.path}")
    else:
        # é»˜è®¤æµ‹è¯•demo_posts
        test_directory("data/demo_posts")


if __name__ == "__main__":
    main()
