#!/usr/bin/env python3
"""
auto_publish_pipeline.py

å®Œæ•´çš„å†…å®¹ç”Ÿæˆå’Œå‘å¸ƒé—­ç¯æµæ°´çº¿
1. è‡ªåŠ¨ç”Ÿæˆå†…å®¹
2. è‡ªåŠ¨å‘å¸ƒåˆ°æ‰€æœ‰å¹³å°ï¼ˆåŒå±‚å¾ªç¯ï¼šæ–‡ç« Ã—å¹³å°ï¼‰
"""

import os
import sys
import time
import argparse
from pathlib import Path
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from generate.auto_content_pipeline import AutoContentPipeline
from src.core.logger import setup_logger
from src.core.session_manager import SessionManager
from src.utils.yaml_file_utils import read_common

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logger('auto_publish_pipeline')


class AutoPublishPipeline:
    """å®Œæ•´çš„å†…å®¹ç”Ÿæˆå’Œå‘å¸ƒæµæ°´çº¿"""
    
    # æ”¯æŒçš„å‘å¸ƒå¹³å°åˆ—è¡¨
    PLATFORMS = ['csdn', 'juejin', 'zhihu', 'cto51', 'alicloud', 'toutiao']
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            api_key: æ™ºè°±AI APIå¯†é’¥
        """
        self.api_key = api_key or os.environ.get("ZHIPUAI_API_KEY")
        if not self.api_key:
            raise ValueError("è¯·æä¾›æ™ºè°±AI API Keyæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPUAI_API_KEY")
        
        # è¯»å–é…ç½®
        self.common_config = read_common()
        self.session_manager = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'generated_articles': 0,
            'total_publishes': 0,
            'success_publishes': 0,
            'failed_publishes': 0,
            'publish_details': []
        }
    
    def get_publisher(self, platform: str):
        """
        æ ¹æ®å¹³å°åç§°è·å–å‘å¸ƒå™¨å®ä¾‹
        
        Args:
            platform: å¹³å°åç§°
        
        Returns:
            å‘å¸ƒå™¨å®ä¾‹
        """
        if platform == 'csdn':
            from src.publisher.csdn_publisher import CSDNPublisher
            return CSDNPublisher()
        elif platform == 'cto51':
            from src.publisher.cto51_publisher import CTO51Publisher
            return CTO51Publisher()
        elif platform == 'toutiao':
            from src.publisher.toutiao_publisher import ToutiaoPublisher
            return ToutiaoPublisher()
        elif platform == 'juejin':
            from src.publisher.juejin_publisher import JuejinPublisher
            return JuejinPublisher()
        elif platform == 'zhihu':
            from src.publisher.zhihu_publisher import ZhihuPublisher
            return ZhihuPublisher()
        elif platform == 'alicloud':
            from src.publisher.alicloud_publisher import AlicloudPublisher
            return AlicloudPublisher()
        else:
            logger.warning(f"å¹³å° {platform} çš„å‘å¸ƒå™¨å°šæœªå®ç°")
            return None
    
    def get_enabled_platforms(self) -> List[str]:
        """
        è·å–å·²å¯ç”¨çš„å¹³å°åˆ—è¡¨
        
        Returns:
            å·²å¯ç”¨çš„å¹³å°åç§°åˆ—è¡¨
        """
        enabled_platforms = self.common_config.get('enable', {})
        return [p for p in self.PLATFORMS if enabled_platforms.get(p, False)]
    
    def publish_article_to_platform(
        self,
        article_path: str,
        platform: str,
        article_index: int,
        total_articles: int,
        platform_index: int,
        total_platforms: int
    ) -> bool:
        """
        å‘å¸ƒå•ç¯‡æ–‡ç« åˆ°æŒ‡å®šå¹³å°
        
        Args:
            article_path: æ–‡ç« è·¯å¾„
            platform: å¹³å°åç§°
            article_index: å½“å‰æ–‡ç« ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
            total_articles: æ–‡ç« æ€»æ•°
            platform_index: å½“å‰å¹³å°ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
            total_platforms: å¹³å°æ€»æ•°
        
        Returns:
            bool: æ˜¯å¦å‘å¸ƒæˆåŠŸ
        """
        article_name = os.path.basename(article_path)
        
        logger.info("\n" + "="*80)
        logger.info(f"ğŸ“„ æ–‡ç«  [{article_index}/{total_articles}]: {article_name}")
        logger.info(f"ğŸš€ å¹³å° [{platform_index}/{total_platforms}]: {platform.upper()}")
        logger.info("="*80)
        
        try:
            publisher = self.get_publisher(platform)
            if not publisher:
                logger.error(f"âŒ æ— æ³•è·å– {platform} çš„å‘å¸ƒå™¨")
                return False
            
            # è®¾ç½®é©±åŠ¨ï¼ˆå¤ç”¨ä¼šè¯ç®¡ç†å™¨ï¼‰
            publisher.session_manager = self.session_manager
            publisher.driver = self.session_manager.driver
            
            # æ‰§è¡Œå‘å¸ƒ
            logger.info(f"â³ å¼€å§‹å‘å¸ƒåˆ° {platform.upper()}...")
            success = publisher.publish(article_path)
            
            if success:
                logger.info(f"âœ… {platform.upper()} å‘å¸ƒæˆåŠŸï¼")
            else:
                logger.error(f"âŒ {platform.upper()} å‘å¸ƒå¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ {platform.upper()} å‘å¸ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{e}", exc_info=True)
            return False
    
    def publish_all_articles(
        self,
        article_paths: List[str],
        platforms: List[str],
        delay_between_publishes: float = 3.0
    ):
        """
        å‘å¸ƒæ‰€æœ‰æ–‡ç« åˆ°æ‰€æœ‰å¹³å°ï¼ˆåŒå±‚å¾ªç¯ï¼‰
        
        Args:
            article_paths: æ–‡ç« è·¯å¾„åˆ—è¡¨
            platforms: å¹³å°åç§°åˆ—è¡¨
            delay_between_publishes: å‘å¸ƒä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        total_articles = len(article_paths)
        total_platforms = len(platforms)
        total_tasks = total_articles * total_platforms
        
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡å‘å¸ƒæµç¨‹")
        logger.info("="*80)
        logger.info(f"ğŸ“Š æ–‡ç« æ•°é‡: {total_articles}")
        logger.info(f"ğŸ“Š å¹³å°æ•°é‡: {total_platforms}")
        logger.info(f"ğŸ“Š æ€»å‘å¸ƒä»»åŠ¡: {total_tasks}")
        logger.info(f"â±ï¸  é¢„è®¡è€—æ—¶: çº¦ {(total_tasks * delay_between_publishes) / 60:.1f} åˆ†é’Ÿ")
        logger.info("="*80 + "\n")
        
        current_task = 0
        
        # å¤–å±‚å¾ªç¯ï¼šéå†æ¯ç¯‡æ–‡ç« 
        for article_idx, article_path in enumerate(article_paths, 1):
            article_name = os.path.basename(article_path)
            
            logger.info("\n" + "ğŸ”¹"*40)
            logger.info(f"ğŸ“ å¼€å§‹å‘å¸ƒæ–‡ç«  [{article_idx}/{total_articles}]: {article_name}")
            logger.info("ğŸ”¹"*40 + "\n")
            
            article_success_count = 0
            article_fail_count = 0
            
            # å†…å±‚å¾ªç¯ï¼šéå†æ¯ä¸ªå¹³å°
            for platform_idx, platform in enumerate(platforms, 1):
                current_task += 1
                
                logger.info(f"\nè¿›åº¦: [{current_task}/{total_tasks}] æ­£åœ¨å‘å¸ƒ...")
                
                # å‘å¸ƒåˆ°å¹³å°
                success = self.publish_article_to_platform(
                    article_path=article_path,
                    platform=platform,
                    article_index=article_idx,
                    total_articles=total_articles,
                    platform_index=platform_idx,
                    total_platforms=total_platforms
                )
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['total_publishes'] += 1
                if success:
                    self.stats['success_publishes'] += 1
                    article_success_count += 1
                else:
                    self.stats['failed_publishes'] += 1
                    article_fail_count += 1
                
                # è®°å½•è¯¦æƒ…
                self.stats['publish_details'].append({
                    'article': article_name,
                    'platform': platform,
                    'success': success
                })
                
                # å»¶è¿Ÿï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
                if current_task < total_tasks:
                    logger.info(f"â³ ç­‰å¾… {delay_between_publishes} ç§’åç»§ç»­...")
                    time.sleep(delay_between_publishes)
            
            # æ–‡ç« å‘å¸ƒå®Œæˆç»Ÿè®¡
            logger.info("\n" + "ğŸ”¹"*40)
            logger.info(f"âœ… æ–‡ç«  [{article_idx}/{total_articles}] å‘å¸ƒå®Œæˆ")
            logger.info(f"   æˆåŠŸ: {article_success_count}/{total_platforms}")
            logger.info(f"   å¤±è´¥: {article_fail_count}/{total_platforms}")
            logger.info("ğŸ”¹"*40 + "\n")
        
        # æœ€ç»ˆç»Ÿè®¡
        self._print_final_report()
    
    def _print_final_report(self):
        """æ‰“å°æœ€ç»ˆå‘å¸ƒæŠ¥å‘Š"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š æ‰¹é‡å‘å¸ƒå®Œæˆ - æ€»ä½“æŠ¥å‘Š")
        logger.info("="*80)
        logger.info(f"æ€»å‘å¸ƒä»»åŠ¡: {self.stats['total_publishes']}")
        logger.info(f"æˆåŠŸ: {self.stats['success_publishes']}")
        logger.info(f"å¤±è´¥: {self.stats['failed_publishes']}")
        
        if self.stats['total_publishes'] > 0:
            success_rate = (self.stats['success_publishes'] / self.stats['total_publishes']) * 100
            logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        logger.info("\n" + "-"*80)
        logger.info("ğŸ“‹ è¯¦ç»†å‘å¸ƒè®°å½•:")
        logger.info("-"*80)
        
        # æŒ‰æ–‡ç« åˆ†ç»„æ˜¾ç¤º
        from collections import defaultdict
        by_article = defaultdict(list)
        for detail in self.stats['publish_details']:
            by_article[detail['article']].append(detail)
        
        for article_name, details in by_article.items():
            logger.info(f"\nğŸ“„ {article_name}")
            for detail in details:
                status = "âœ…" if detail['success'] else "âŒ"
                logger.info(f"   {status} {detail['platform'].upper()}")
        
        logger.info("\n" + "="*80 + "\n")
    
    def initialize_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        logger.info("ğŸŒ åˆå§‹åŒ–æµè§ˆå™¨...")
        
        try:
            self.session_manager = SessionManager('common', self.common_config)
            self.session_manager.create_driver(use_existing=True)
            logger.info("âœ… æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ è¿æ¥Chromeå¤±è´¥ï¼š{error_msg}")
            
            if 'cannot connect to chrome' in error_msg.lower() or 'unable to discover open pages' in error_msg.lower():
                logger.error("\n" + "="*80)
                logger.error("âš ï¸  æ— æ³•è¿æ¥åˆ° Chrome è°ƒè¯•æ¨¡å¼")
                logger.error("="*80)
                logger.error("\nè¯·å…ˆå¯åŠ¨ Chrome è°ƒè¯•æ¨¡å¼ï¼š")
                logger.error("\næ–¹æ³•1ï¼šä½¿ç”¨è„šæœ¬å¯åŠ¨ï¼ˆæ¨èï¼‰")
                logger.error("  bash scripts/start_chrome.sh")
                logger.error("\næ–¹æ³•2ï¼šæ‰‹åŠ¨å¯åŠ¨")
                logger.error("  /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\")
                logger.error("    --remote-debugging-port=9222 \\")
                logger.error("    --user-data-dir=\"/tmp/chrome_dev\" \\")
                logger.error("    about:blank")
                logger.error("\n" + "="*80 + "\n")
            
            return False
    
    def run(
        self,
        news_limit: int = 10,
        article_limit: int = 5,
        search_depth: str = "quick",
        delay: float = 2.0,
        publish_delay: float = 3.0,
        skip_generation: bool = False
    ):
        """
        è¿è¡Œå®Œæ•´æµæ°´çº¿
        
        Args:
            news_limit: æŠ“å–çš„æ–°é—»æ•°é‡
            article_limit: ç”Ÿæˆçš„æ–‡ç« æ•°é‡
            search_depth: æœç´¢æ·±åº¦ "quick" æˆ– "deep"
            delay: å†…å®¹ç”Ÿæˆæ—¶çš„APIè¯·æ±‚å»¶è¿Ÿ
            publish_delay: å‘å¸ƒæ—¶çš„å»¶è¿Ÿ
            skip_generation: æ˜¯å¦è·³è¿‡å†…å®¹ç”Ÿæˆï¼ˆç›´æ¥å‘å¸ƒå·²æœ‰æ–‡ç« ï¼‰
        """
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆä¸å‘å¸ƒæµæ°´çº¿")
        logger.info("="*80 + "\n")
        
        article_paths = []
        
        try:
            # æ­¥éª¤1ï¼šç”Ÿæˆå†…å®¹
            if not skip_generation:
                logger.info("ğŸ“ [é˜¶æ®µ 1/2] å†…å®¹ç”Ÿæˆ")
                logger.info("-"*80 + "\n")
                
                content_pipeline = AutoContentPipeline(
                    api_key=self.api_key,
                    output_dir="posts",
                    data_dir="data/generated"
                )
                
                stats = content_pipeline.run(
                    news_limit=news_limit,
                    article_limit=article_limit,
                    search_depth=search_depth,
                    request_delay=delay,
                    save_intermediate=True
                )
                
                self.stats['generated_articles'] = stats['generated_articles']
                
                if stats['generated_articles'] == 0:
                    logger.error("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•æ–‡ç« ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                    return
                
                # è·å–ç”Ÿæˆçš„æ–‡ç« è·¯å¾„
                import json
                generated_file = Path("data/generated/03_generated_articles.json")
                if generated_file.exists():
                    with open(generated_file, 'r', encoding='utf-8') as f:
                        generated_articles = json.load(f)
                        article_paths = [article['file_path'] for article in generated_articles]
                
                logger.info(f"\nâœ… å†…å®¹ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(article_paths)} ç¯‡æ–‡ç« \n")
            
            else:
                # è·³è¿‡ç”Ÿæˆï¼Œä½¿ç”¨ç°æœ‰æ–‡ç« 
                logger.info("â­ï¸  è·³è¿‡å†…å®¹ç”Ÿæˆï¼Œä½¿ç”¨ç°æœ‰æ–‡ç« ")
                posts_dir = Path("posts")
                if posts_dir.exists():
                    article_paths = [str(f) for f in posts_dir.glob("*.md")]
                    logger.info(f"âœ… æ‰¾åˆ° {len(article_paths)} ç¯‡ç°æœ‰æ–‡ç« \n")
                else:
                    logger.error("âŒ posts ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ–‡ç« å¯å‘å¸ƒ")
                    return
            
            if not article_paths:
                logger.error("âŒ æ²¡æœ‰å¯å‘å¸ƒçš„æ–‡ç« ")
                return
            
            # æ­¥éª¤2ï¼šåˆå§‹åŒ–æµè§ˆå™¨
            logger.info("\n" + "="*80)
            logger.info("ğŸŒ [é˜¶æ®µ 2/3] åˆå§‹åŒ–æµè§ˆå™¨")
            logger.info("-"*80 + "\n")
            
            if not self.initialize_browser():
                logger.error("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å‘å¸ƒ")
                return
            
            # æ­¥éª¤3ï¼šæ‰¹é‡å‘å¸ƒ
            logger.info("\n" + "="*80)
            logger.info("ğŸ“¤ [é˜¶æ®µ 3/3] æ‰¹é‡å‘å¸ƒ")
            logger.info("-"*80 + "\n")
            
            # è·å–å¯ç”¨çš„å¹³å°
            enabled_platforms = self.get_enabled_platforms()
            
            if not enabled_platforms:
                logger.error("âŒ æ²¡æœ‰å¯ç”¨ä»»ä½•å‘å¸ƒå¹³å°ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
                logger.info("   é…ç½®æ–‡ä»¶è·¯å¾„: config/common.yaml")
                logger.info("   è¯·åœ¨ enable éƒ¨åˆ†å¯ç”¨éœ€è¦çš„å¹³å°")
                return
            
            logger.info(f"ğŸ“‹ å¯ç”¨çš„å¹³å°: {', '.join(p.upper() for p in enabled_platforms)}")
            
            # æ‰§è¡Œæ‰¹é‡å‘å¸ƒ
            self.publish_all_articles(
                article_paths=article_paths,
                platforms=enabled_platforms,
                delay_between_publishes=publish_delay
            )
            
            logger.info("\nğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
            
        except KeyboardInterrupt:
            logger.info("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æµæ°´çº¿")
        except Exception as e:
            logger.error(f"\n\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        finally:
            # æ¸…ç†èµ„æº
            if self.session_manager:
                self.session_manager.close()
            logger.info("\nğŸ‘‹ ç¨‹åºé€€å‡º\n")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='å®Œæ•´çš„å†…å®¹ç”Ÿæˆå’Œå‘å¸ƒé—­ç¯æµæ°´çº¿'
    )
    
    parser.add_argument(
        '--news-limit',
        type=int,
        default=10,
        help='æŠ“å–çš„æ–°é—»æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰'
    )
    
    parser.add_argument(
        '--article-limit',
        type=int,
        default=1,
        help='ç”Ÿæˆçš„æ–‡ç« æ•°é‡ï¼ˆé»˜è®¤: 1ï¼‰'
    )
    
    parser.add_argument(
        '--search-depth',
        choices=['quick', 'deep'],
        default='quick',
        help='æœç´¢æ·±åº¦ï¼šquick=å¿«é€Ÿ, deep=æ·±åº¦ï¼ˆé»˜è®¤: quickï¼‰'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=2.0,
        help='å†…å®¹ç”Ÿæˆæ—¶çš„APIè¯·æ±‚é—´éš”ç§’æ•°ï¼ˆé»˜è®¤: 2.0ï¼‰'
    )
    
    parser.add_argument(
        '--publish-delay',
        type=float,
        default=3.0,
        help='å‘å¸ƒæ—¶çš„å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: 3.0ï¼‰'
    )
    
    parser.add_argument(
        '--skip-generation',
        action='store_true',
        help='è·³è¿‡å†…å®¹ç”Ÿæˆï¼Œç›´æ¥å‘å¸ƒå·²æœ‰æ–‡ç« '
    )
    
    parser.add_argument(
        '--api-key',
        help='æ™ºè°±AI APIå¯†é’¥ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡ZHIPUAI_API_KEYè®¾ç½®ï¼‰'
    )
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæµæ°´çº¿
        pipeline = AutoPublishPipeline(api_key=args.api_key)
        
        # è¿è¡Œæµæ°´çº¿
        pipeline.run(
            news_limit=args.news_limit,
            article_limit=args.article_limit,
            search_depth=args.search_depth,
            delay=args.delay,
            publish_delay=args.publish_delay,
            skip_generation=args.skip_generation
        )
        
    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
