#!/usr/bin/env python3
"""
auto_content_pipeline.py

è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆæµæ°´çº¿
ä»æŠ“å–çƒ­ç‚¹ -> æœç´¢èµ„æ–™ -> ç”Ÿæˆæ–‡ç«  -> ä¿å­˜å‘å¸ƒçš„å®Œæ•´æµç¨‹
"""

import os
import sys
import json
import time
import argparse
import yaml
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from generate.qbitai_crawler import QbitAICrawler
from generate.aibase_crawler import AIBaseCrawler
from generate.reference_searcher import ReferenceSearcher
from generate.enhanced_content_generator import EnhancedContentGenerator


class AutoContentPipeline:
    """è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆæµæ°´çº¿"""
    
    # æ–°é—»æºæ˜ å°„
    CRAWLER_MAP = {
        'qbitai': QbitAICrawler,
        'aibase': AIBaseCrawler
    }
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        output_dir: str = "posts",
        data_dir: str = "data/generated",
        news_sources: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            api_key: æ™ºè°±AI APIå¯†é’¥
            output_dir: æ–‡ç« è¾“å‡ºç›®å½•
            data_dir: ä¸­é—´æ•°æ®ä¿å­˜ç›®å½•
            news_sources: æ–°é—»æºé…ç½®ï¼Œé€—å·åˆ†éš”ï¼ˆå¦‚ "aibase,qbitai"ï¼‰ï¼Œä¸ºç©ºåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–
        """
        self.api_key = api_key or os.environ.get("ZHIPUAI_API_KEY")
        if not self.api_key:
            raise ValueError("è¯·æä¾›æ™ºè°±AI API Keyæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPUAI_API_KEY")
        
        self.output_dir = output_dir
        self.data_dir = data_dir
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        Path(self.data_dir).mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ–°é—»æºé…ç½®
        self.news_sources = self._load_news_sources(news_sources)
        
        # åˆå§‹åŒ–çˆ¬è™«åˆ—è¡¨
        self.crawlers = self._init_crawlers()
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        self.searcher = ReferenceSearcher(api_key=self.api_key)
        self.generator = EnhancedContentGenerator(api_key=self.api_key)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'crawled_news': 0,
            'searched_references': 0,
            'generated_articles': 0,
            'failed_articles': 0,
            'start_time': None,
            'end_time': None,
            'news_sources': self.news_sources
        }
    
    def _load_news_sources(self, news_sources: Optional[str]) -> List[str]:
        """
        åŠ è½½æ–°é—»æºé…ç½®
        
        Args:
            news_sources: å‘½ä»¤è¡ŒæŒ‡å®šçš„æ–°é—»æº
            
        Returns:
            æ–°é—»æºåˆ—è¡¨
        """
        # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†æ–°é—»æºï¼Œä½¿ç”¨å‘½ä»¤è¡Œçš„
        if news_sources:
            sources = [s.strip().lower() for s in news_sources.split(',')]
        else:
            # å¦åˆ™ä»é…ç½®æ–‡ä»¶è¯»å–
            try:
                config_path = Path(__file__).parent.parent / "config" / "common.yaml"
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    sources_str = config.get('news_sources', 'aibase')
                    sources = [s.strip().lower() for s in sources_str.split(',')]
            except Exception as e:
                print(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ–°é—»æº: aibase")
                sources = ['aibase']
        
        # éªŒè¯æ–°é—»æºæ˜¯å¦æ”¯æŒ
        valid_sources = []
        for source in sources:
            if source in self.CRAWLER_MAP:
                valid_sources.append(source)
            else:
                print(f"âš ï¸ ä¸æ”¯æŒçš„æ–°é—»æº: {source}ï¼Œå·²è·³è¿‡")
        
        if not valid_sources:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ–°é—»æºï¼Œä½¿ç”¨é»˜è®¤: aibase")
            valid_sources = ['aibase']
        
        return valid_sources
    
    def _init_crawlers(self) -> List:
        """
        åˆå§‹åŒ–çˆ¬è™«å®ä¾‹
        
        Returns:
            çˆ¬è™«å®ä¾‹åˆ—è¡¨
        """
        crawlers = []
        for source in self.news_sources:
            crawler_class = self.CRAWLER_MAP.get(source)
            if crawler_class:
                crawlers.append({
                    'name': source,
                    'instance': crawler_class()
                })
        return crawlers
    
    def run(
        self,
        news_limit: int = 10,
        article_limit: int = 5,
        search_depth: str = "quick",
        request_delay: float = 2.0,
        save_intermediate: bool = True
    ) -> Dict[str, any]:
        """
        è¿è¡Œå®Œæ•´æµæ°´çº¿
        
        Args:
            news_limit: æ¯ä¸ªæ–°é—»æºæŠ“å–çš„æ–°é—»æ•°é‡
            article_limit: ç”Ÿæˆçš„æ–‡ç« æ•°é‡ï¼ˆä»æ‰€æœ‰æŠ“å–çš„æ–°é—»ä¸­é€‰å–ï¼‰
            search_depth: æœç´¢æ·±åº¦ "quick" æˆ– "deep"
            request_delay: APIè¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        self.stats['start_time'] = datetime.now()
        
        print("\n" + "="*80)
        print("ğŸš€ è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆæµæ°´çº¿å¯åŠ¨")
        print("="*80)
        print(f"æ–°é—»æº: {', '.join(self.news_sources)}")
        print(f"æ¯ä¸ªæºæŠ“å–æ–°é—»æ•°: {news_limit}")
        print(f"ç”Ÿæˆæ–‡ç« æ•°: {article_limit}")
        print(f"æœç´¢æ·±åº¦: {search_depth}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print("="*80 + "\n")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šä»å¤šä¸ªæ–°é—»æºæŠ“å–çƒ­ç‚¹æ–°é—»
            print("\n" + "="*80)
            print("ğŸ“¡ [æ­¥éª¤ 1/4] æŠ“å–çƒ­ç‚¹æ–°é—»")
            print("="*80 + "\n")
            
            all_news = []
            for crawler_info in self.crawlers:
                source_name = crawler_info['name']
                crawler = crawler_info['instance']
                
                print(f"\nğŸ“° æ­£åœ¨æŠ“å– {source_name.upper()} æ–°é—»æº...")
                news_list = crawler.fetch_top_news(limit=news_limit)
                
                # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ æ¥æºæ ‡è®°
                for news in news_list:
                    news['news_source'] = source_name
                
                all_news.extend(news_list)
                print(f"âœ… {source_name.upper()}: æˆåŠŸæŠ“å– {len(news_list)} æ¡æ–°é—»")
            
            self.stats['crawled_news'] = len(all_news)
            
            if not all_news:
                print("âŒ æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return self.stats
            
            if save_intermediate:
                self._save_json(all_news, "01_crawled_news.json")
            
            print(f"\nâœ… æ€»è®¡æŠ“å– {len(all_news)} æ¡æ–°é—»")
            print(f"   æ¥æºåˆ†å¸ƒ: {self._count_by_source(all_news)}\n")
            
            # é€‰æ‹©è¦ç”Ÿæˆæ–‡ç« çš„æ–°é—»ï¼ˆå–å‰ article_limit æ¡ï¼‰
            selected_news = all_news[:article_limit]
            print(f"ğŸ“Œ é€‰æ‹©å‰ {len(selected_news)} æ¡æ–°é—»ç”Ÿæˆæ–‡ç« \n")
            
            # ç¬¬äºŒæ­¥ï¼šæœç´¢å‚è€ƒèµ„æ–™
            print("\n" + "="*80)
            print("ğŸ” [æ­¥éª¤ 2/4] æœç´¢å‚è€ƒèµ„æ–™")
            print("="*80 + "\n")
            
            all_references = []
            for idx, news in enumerate(selected_news, 1):
                print(f"\n--- [{idx}/{len(selected_news)}] æœç´¢: {news['title'][:50]}... ---")
                
                try:
                    references = self.searcher.search_topic_references(
                        topic=news['title'],
                        original_summary=news.get('summary', ''),
                        search_depth=search_depth
                    )
                    all_references.append(references)
                    self.stats['searched_references'] += 1
                    
                    # APIè¯·æ±‚é™æµ
                    if idx < len(selected_news):
                        print(f"\nâ³ ç­‰å¾… {request_delay} ç§’...")
                        time.sleep(request_delay)
                        
                except Exception as e:
                    print(f"âŒ æœç´¢å¤±è´¥: {e}")
                    all_references.append({
                        'topic': news['title'],
                        'error': str(e)
                    })
            
            if save_intermediate:
                self._save_json(all_references, "02_search_references.json")
            
            print(f"\nâœ… å®Œæˆ {len(all_references)} ä¸ªè¯é¢˜çš„èµ„æ–™æœç´¢\n")
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ–‡ç« 
            print("\n" + "="*80)
            print("âœï¸  [æ­¥éª¤ 3/4] ç”Ÿæˆæ–‡ç« ")
            print("="*80 + "\n")
            
            generated_articles = []
            for idx, (news, references) in enumerate(zip(selected_news, all_references), 1):
                print(f"\n--- [{idx}/{len(selected_news)}] ç”Ÿæˆæ–‡ç«  ---")
                
                try:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢é”™è¯¯
                    if 'error' in references:
                        print(f"âš ï¸ è·³è¿‡ï¼ˆæœç´¢å¤±è´¥ï¼‰: {news['title'][:50]}...")
                        self.stats['failed_articles'] += 1
                        continue
                    
                    article = self.generator.generate_article_from_news(
                        news_item=news,
                        references=references,
                        style="qbitai",
                        output_dir=self.output_dir
                    )
                    
                    generated_articles.append({
                        'title': article['title'],
                        'file_path': article['file_path'],
                        'tags': article['tags'],
                        'original_title': news['title']
                    })
                    
                    self.stats['generated_articles'] += 1
                    
                    # APIè¯·æ±‚é™æµ
                    if idx < len(selected_news):
                        print(f"\nâ³ ç­‰å¾… {request_delay} ç§’...")
                        time.sleep(request_delay)
                    
                except Exception as e:
                    print(f"âŒ æ–‡ç« ç”Ÿæˆå¤±è´¥: {e}")
                    self.stats['failed_articles'] += 1
            
            if save_intermediate:
                self._save_json(generated_articles, "03_generated_articles.json")
            
            # ç¬¬å››æ­¥ï¼šç”ŸæˆæŠ¥å‘Š
            print("\n" + "="*80)
            print("ğŸ“Š [æ­¥éª¤ 4/4] ç”Ÿæˆè¿è¡ŒæŠ¥å‘Š")
            print("="*80 + "\n")
            
            self.stats['end_time'] = datetime.now()
            duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
            self.stats['duration_seconds'] = duration
            
            report = self._generate_report(generated_articles)
            
            if save_intermediate:
                self._save_text(report, "04_pipeline_report.txt")
            
            print(report)
            
            return self.stats
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµæ°´çº¿")
            return self.stats
        except Exception as e:
            print(f"\n\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self.stats
    
    def _save_json(self, data: any, filename: str):
        """ä¿å­˜JSONæ•°æ®"""
        file_path = Path(self.data_dir) / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¸­é—´æ•°æ®å·²ä¿å­˜: {file_path}")
    
    def _save_text(self, text: str, filename: str):
        """ä¿å­˜æ–‡æœ¬æ•°æ®"""
        file_path = Path(self.data_dir) / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        print(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {file_path}")
    
    def _count_by_source(self, news_list: List[Dict]) -> str:
        """
        ç»Ÿè®¡å„æ–°é—»æºçš„æ–°é—»æ•°é‡
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ç»Ÿè®¡å­—ç¬¦ä¸²
        """
        counts = {}
        for news in news_list:
            source = news.get('news_source', 'unknown')
            counts[source] = counts.get(source, 0) + 1
        
        return ', '.join([f"{k}={v}" for k, v in counts.items()])
    
    def _generate_report(self, articles: List[Dict]) -> str:
        """ç”Ÿæˆè¿è¡ŒæŠ¥å‘Š"""
        duration_minutes = self.stats['duration_seconds'] / 60
        
        report = f"""
{'='*80}
ğŸ“Š å†…å®¹ç”Ÿæˆæµæ°´çº¿è¿è¡ŒæŠ¥å‘Š
{'='*80}

â° è¿è¡Œæ—¶é—´
   å¼€å§‹æ—¶é—´: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}
   ç»“æŸæ—¶é—´: {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
   æ€»è€—æ—¶: {duration_minutes:.2f} åˆ†é’Ÿ

ğŸŒ æ–°é—»æºé…ç½®
   ä½¿ç”¨çš„æ–°é—»æº: {', '.join(self.stats['news_sources'])}

ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯
   æŠ“å–æ–°é—»: {self.stats['crawled_news']} æ¡
   æœç´¢èµ„æ–™: {self.stats['searched_references']} ä¸ªè¯é¢˜
   æˆåŠŸç”Ÿæˆ: {self.stats['generated_articles']} ç¯‡æ–‡ç« 
   å¤±è´¥æ•°é‡: {self.stats['failed_articles']} ç¯‡
   æˆåŠŸç‡: {self.stats['generated_articles']/(self.stats['generated_articles']+self.stats['failed_articles'])*100 if (self.stats['generated_articles']+self.stats['failed_articles'])>0 else 0:.1f}%

ğŸ“ ç”Ÿæˆçš„æ–‡ç« åˆ—è¡¨
"""
        
        if articles:
            for idx, article in enumerate(articles, 1):
                report += f"""
   [{idx}] {article['title']}
       åŸæ ‡é¢˜: {article['original_title']}
       æ ‡ç­¾: {', '.join(article['tags'])}
       æ–‡ä»¶: {article['file_path']}
"""
        else:
            report += "   ï¼ˆæ— ï¼‰\n"
        
        report += f"""
{'='*80}
âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ
{'='*80}
"""
        
        return report


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆæµæ°´çº¿ï¼šæŠ“å– -> æœç´¢ -> ç”Ÿæˆ -> å‘å¸ƒ'
    )
    
    parser.add_argument(
        '--news-limit',
        type=int,
        default=10,
        help='æ¯ä¸ªæ–°é—»æºæŠ“å–çš„æ–°é—»æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰'
    )
    
    parser.add_argument(
        '--article-limit',
        type=int,
        default=1,
        help='ç”Ÿæˆçš„æ–‡ç« æ•°é‡ï¼ˆé»˜è®¤: 1ï¼‰'
    )
    
    parser.add_argument(
        '--search-depth',
        choices=['quick', 'deep'],
        default='quick',
        help='æœç´¢æ·±åº¦ï¼šquick=å¿«é€Ÿ, deep=æ·±åº¦ï¼ˆé»˜è®¤: quickï¼‰'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=2.0,
        help='APIè¯·æ±‚é—´éš”ç§’æ•°ï¼ˆé»˜è®¤: 2.0ï¼‰'
    )
    
    parser.add_argument(
        '--output-dir',
        default='posts',
        help='æ–‡ç« è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: postsï¼‰'
    )
    
    parser.add_argument(
        '--data-dir',
        default='data/generated',
        help='ä¸­é—´æ•°æ®ç›®å½•ï¼ˆé»˜è®¤: data/generatedï¼‰'
    )
    
    parser.add_argument(
        '--api-key',
        help='æ™ºè°±AI APIå¯†é’¥ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡ZHIPUAI_API_KEYè®¾ç½®ï¼‰'
    )
    
    parser.add_argument(
        '--news-sources',
        help='æ–°é—»æºï¼Œé€—å·åˆ†éš”ï¼ˆå¦‚: aibase,qbitaiï¼‰ã€‚ä¸æŒ‡å®šåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–'
    )
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæµæ°´çº¿
        pipeline = AutoContentPipeline(
            api_key=args.api_key,
            output_dir=args.output_dir,
            data_dir=args.data_dir,
            news_sources=args.news_sources
        )
        
        # è¿è¡Œæµæ°´çº¿
        stats = pipeline.run(
            news_limit=args.news_limit,
            article_limit=args.article_limit,
            search_depth=args.search_depth,
            request_delay=args.delay,
            save_intermediate=True
        )
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print("\n" + "="*80)
        print("ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
        print("="*80)
        print(f"æˆåŠŸç”Ÿæˆ {stats['generated_articles']} ç¯‡æ–‡ç« ")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
