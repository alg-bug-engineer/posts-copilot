#!/usr/bin/env python3
"""
aibase_crawler.py

ä» AIBase (aibase.com) æŠ“å–æœ€æ–° AI æ–°é—»
æå–æ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦ã€æ—¶é—´ç­‰ä¿¡æ¯
"""

import re
import time
from typing import List, Dict, Optional
from datetime import datetime
import requests
from bs4 import BeautifulSoup


class AIBaseCrawler:
    """AIBase æ–°é—»çˆ¬è™«"""
    
    BASE_URL = "https://www.aibase.com/zh/news"
    
    # è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0',
        'Referer': 'https://www.aibase.com/'
    }
    
    def __init__(self, timeout: int = 10):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)
    
    def fetch_top_news(self, limit: int = 10) -> List[Dict[str, str]]:
        """
        æŠ“å– AIBase æœ€æ–°æ–°é—»
        
        Args:
            limit: æŠ“å–çš„æ–°é—»æ•°é‡ï¼Œé»˜è®¤10æ¡
            
        Returns:
            æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«ï¼š
            {
                'title': æ ‡é¢˜,
                'url': é“¾æ¥,
                'summary': æ‘˜è¦,
                'time': å‘å¸ƒæ—¶é—´,
                'source': æ¥æº,
                'image_url': å°é¢å›¾ç‰‡
            }
        """
        print(f"\n{'='*70}")
        print(f"ğŸš€ å¼€å§‹æŠ“å– AIBase æœ€æ–° AI æ–°é—» TOP{limit}")
        print(f"{'='*70}\n")
        
        try:
            # å‘èµ·è¯·æ±‚
            print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚: {self.BASE_URL}")
            response = self.session.get(self.BASE_URL, timeout=self.timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–°é—»é“¾æ¥ <a> æ ‡ç­¾
            # æ ¹æ®æä¾›çš„HTMLç»“æ„ï¼Œæ–°é—»é¡¹æ˜¯ <a> æ ‡ç­¾ï¼ŒclassåŒ…å« "flex group justify-between"
            articles = soup.find_all(
                'a',
                class_=lambda x: x and 'flex' in x and 'group' in x and 'justify-between' in x,
                limit=limit
            )
            
            if not articles:
                print("âŒ æœªæ‰¾åˆ°æ–°é—»åˆ—è¡¨")
                return []
            
            print(f"âœ“ æ‰¾åˆ° {len(articles)} ç¯‡æ–°é—»\n")
            
            news_list = []
            for idx, article in enumerate(articles, 1):
                try:
                    news_item = self._parse_article(article, idx)
                    if news_item:
                        news_list.append(news_item)
                        print(f"  [{idx}] âœ“ {news_item['title'][:50]}...")
                except Exception as e:
                    print(f"  [{idx}] âœ— è§£æå¤±è´¥: {e}")
                    continue
            
            print(f"\n{'='*70}")
            print(f"âœ… æˆåŠŸæŠ“å– {len(news_list)} æ¡æ–°é—»")
            print(f"{'='*70}\n")
            
            return news_list
            
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"âŒ æŠ“å–å‡ºé”™: {e}")
            return []
    
    def _parse_article(self, article_element, index: int) -> Optional[Dict[str, str]]:
        """
        è§£æå•ä¸ªæ–°é—»æ¡ç›®
        
        Args:
            article_element: BeautifulSoupå…ƒç´ å¯¹è±¡
            index: æ–°é—»åºå·
            
        Returns:
            è§£æåçš„æ–°é—»ä¿¡æ¯å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # æå–é“¾æ¥
            url = article_element.get('href', '')
            if url and not url.startswith('http'):
                url = f"https://www.aibase.com{url}"
            
            # æå–æ ‡é¢˜ <h3> æ ‡ç­¾
            title_elem = article_element.find('h3', class_=lambda x: x and 'line-clamp-2' in x)
            title = title_elem.get_text(strip=True) if title_elem else ""
            
            # æå–æ‘˜è¦ <div> æ ‡ç­¾ï¼Œclass åŒ…å« "line-clamp-2 text-surface-500"
            summary_elem = article_element.find(
                'div',
                class_=lambda x: x and 'line-clamp-2' in x and 'text-surface-500' in x
            )
            summary = summary_elem.get_text(strip=True) if summary_elem else ""
            
            # æå–æ—¶é—´å’Œæ¥æº
            # <div class="text-sm text-gray-400 flex items-center space-x-1">
            #   <span>11  åˆ†é’Ÿå‰</span><span>.</span><span class="font-light">AIbase</span>
            # </div>
            time_info_div = article_element.find('div', class_=lambda x: x and 'text-gray-400' in x)
            time_str = ""
            source = "AIbase"
            
            if time_info_div:
                spans = time_info_div.find_all('span')
                if len(spans) >= 1:
                    time_str = spans[0].get_text(strip=True)
                if len(spans) >= 3:
                    source = spans[2].get_text(strip=True)
            
            # æå–å›¾ç‰‡
            img_elem = article_element.find('img')
            image_url = img_elem.get('src', '') if img_elem else ""
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not title or not url:
                return None
            
            return {
                'title': title,
                'url': url,
                'summary': summary,
                'time': time_str,
                'source': source,
                'image_url': image_url,
                'crawled_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"  è§£ææ–‡ç«  #{index} å¤±è´¥: {e}")
            return None
    
    def get_news_detail(self, url: str) -> Optional[Dict[str, str]]:
        """
        è·å–æ–°é—»è¯¦æƒ…é¡µå†…å®¹
        
        Args:
            url: æ–°é—»è¯¦æƒ…é¡µé“¾æ¥
            
        Returns:
            åŒ…å«è¯¦ç»†å†…å®¹çš„å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…çš„è¯¦æƒ…é¡µç»“æ„æå–æ›´å¤šä¿¡æ¯
            # ç›®å‰è¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                'full_content': soup.get_text(strip=True),
                'fetched_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"è·å–è¯¦æƒ…é¡µå¤±è´¥ {url}: {e}")
            return None


def main():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    crawler = AIBaseCrawler()
    
    # æŠ“å–å‰10æ¡æ–°é—»
    news_list = crawler.fetch_top_news(limit=10)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“° æŠ“å–ç»“æœ:")
    print("="*80 + "\n")
    
    for idx, news in enumerate(news_list, 1):
        print(f"{idx}. {news['title']}")
        print(f"   æ—¶é—´: {news['time']}")
        print(f"   æ¥æº: {news['source']}")
        print(f"   é“¾æ¥: {news['url']}")
        if news.get('summary'):
            print(f"   æ‘˜è¦: {news['summary'][:100]}...")
        print()


if __name__ == "__main__":
    main()
