#!/usr/bin/env python3
"""
qbitai_crawler.py

ä»é‡å­ä½(qbitai.com)æŠ“å–çƒ­é—¨ç§‘æŠ€æ–°é—»
æå–æ ‡é¢˜ã€é“¾æ¥ã€ç®€ä»‹ã€æ ‡ç­¾ã€ä½œè€…ã€æ—¶é—´ç­‰ä¿¡æ¯
"""

import re
import time
from typing import List, Dict, Optional
from datetime import datetime
import requests
from bs4 import BeautifulSoup


class QbitAICrawler:
    """é‡å­ä½æ–°é—»çˆ¬è™«"""
    
    BASE_URL = "https://www.qbitai.com"
    
    # è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0'
    }
    
    def __init__(self, timeout: int = 10):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)
    
    def fetch_top_news(self, limit: int = 10) -> List[Dict[str, str]]:
        """
        æŠ“å–é‡å­ä½é¦–é¡µTOPçƒ­ç‚¹æ–°é—»
        
        Args:
            limit: æŠ“å–çš„æ–°é—»æ•°é‡ï¼Œé»˜è®¤10æ¡
            
        Returns:
            æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«ï¼š
            {
                'title': æ ‡é¢˜,
                'url': é“¾æ¥,
                'summary': æ‘˜è¦,
                'author': ä½œè€…,
                'time': å‘å¸ƒæ—¶é—´,
                'tags': æ ‡ç­¾åˆ—è¡¨,
                'image_url': å°é¢å›¾ç‰‡
            }
        """
        print(f"\n{'='*70}")
        print(f"ğŸš€ å¼€å§‹æŠ“å–é‡å­ä½é¦–é¡µ TOP{limit} çƒ­ç‚¹æ–°é—»")
        print(f"{'='*70}\n")
        
        try:
            # å‘èµ·è¯·æ±‚
            print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚: {self.BASE_URL}")
            response = self.session.get(self.BASE_URL, timeout=self.timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨å®¹å™¨
            article_list = soup.find('div', class_='article_list')
            if not article_list:
                print("âŒ æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨å®¹å™¨")
                return []
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« é¡¹
            articles = article_list.find_all('div', class_='picture_text', limit=limit)
            print(f"âœ“ æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« \n")
            
            news_list = []
            for idx, article in enumerate(articles, 1):
                try:
                    news_item = self._parse_article(article, idx)
                    if news_item:
                        news_list.append(news_item)
                        print(f"  [{idx}] âœ“ {news_item['title'][:50]}...")
                except Exception as e:
                    print(f"  [{idx}] âœ— è§£æå¤±è´¥: {e}")
                    continue
            
            print(f"\n{'='*70}")
            print(f"âœ… æˆåŠŸæŠ“å– {len(news_list)} æ¡æ–°é—»")
            print(f"{'='*70}\n")
            
            return news_list
            
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"âŒ æŠ“å–è¿‡ç¨‹å‡ºé”™: {e}")
            return []
    
    def _parse_article(self, article_element, index: int) -> Optional[Dict[str, str]]:
        """
        è§£æå•ç¯‡æ–‡ç« å…ƒç´ 
        
        Args:
            article_element: BeautifulSoupæ–‡ç« å…ƒç´ 
            index: æ–‡ç« åºå·
            
        Returns:
            æ–‡ç« ä¿¡æ¯å­—å…¸ï¼Œè§£æå¤±è´¥è¿”å›None
        """
        # æå–å›¾ç‰‡
        image_url = ""
        picture_div = article_element.find('div', class_='picture')
        if picture_div:
            img_tag = picture_div.find('img')
            if img_tag and img_tag.get('src'):
                image_url = img_tag['src']
        
        # æå–æ–‡æœ¬åŒºåŸŸ
        text_box = article_element.find('div', class_='text_box')
        if not text_box:
            return None
        
        # æå–æ ‡é¢˜å’Œé“¾æ¥
        h4_tag = text_box.find('h4')
        if not h4_tag:
            return None
        
        a_tag = h4_tag.find('a')
        if not a_tag:
            return None
        
        title = a_tag.get_text(strip=True)
        url = a_tag.get('href', '')
        if not url.startswith('http'):
            url = self.BASE_URL + url
        
        # æå–æ‘˜è¦
        summary = ""
        summary_p = text_box.find('p')
        if summary_p:
            summary_text = summary_p.get_text(strip=True)
            # è¿‡æ»¤æ‰ç©ºå†…å®¹
            if summary_text and summary_text not in ['""', '']:
                summary = summary_text
        
        # æå–ä¿¡æ¯åŒºåŸŸï¼ˆä½œè€…ã€æ—¶é—´ã€æ ‡ç­¾ï¼‰
        info_div = text_box.find('div', class_='info')
        author = ""
        publish_time = ""
        tags = []
        
        if info_div:
            # æå–ä½œè€…
            author_span = info_div.find('span', class_='author')
            if author_span:
                author_a = author_span.find('a')
                if author_a:
                    author = author_a.get_text(strip=True)
            
            # æå–æ—¶é—´
            time_span = info_div.find('span', class_='time')
            if time_span:
                publish_time = time_span.get_text(strip=True)
            
            # æå–æ ‡ç­¾
            tags_div = info_div.find('div', class_='tags_s')
            if tags_div:
                tag_links = tags_div.find_all('a', rel='tag')
                tags = [tag.get_text(strip=True) for tag in tag_links if tag.get_text(strip=True)]
        
        return {
            'title': title,
            'url': url,
            'summary': summary if summary else title,  # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œä½¿ç”¨æ ‡é¢˜
            'author': author,
            'time': publish_time,
            'tags': tags,
            'image_url': image_url,
            'source': 'é‡å­ä½',
            'crawled_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def fetch_article_detail(self, url: str) -> Optional[Dict[str, str]]:
        """
        æŠ“å–æ–‡ç« è¯¦æƒ…é¡µå†…å®¹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            åŒ…å«å®Œæ•´å†…å®¹çš„å­—å…¸
        """
        try:
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–æ–‡ç« è¯¦æƒ…: {url}")
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•æå–æ–‡ç« æ­£æ–‡ï¼ˆè¿™éƒ¨åˆ†éœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´ï¼‰
            article_content = soup.find('div', class_='article-content')
            if not article_content:
                article_content = soup.find('article')
            
            if article_content:
                # æå–æ‰€æœ‰æ®µè½
                paragraphs = article_content.find_all(['p', 'h2', 'h3'])
                content = '\n\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                
                return {
                    'url': url,
                    'content': content,
                    'fetched_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°æ–‡ç« æ­£æ–‡åŒºåŸŸ")
                return None
                
        except Exception as e:
            print(f"  âŒ æŠ“å–è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def save_to_json(self, news_list: List[Dict], output_file: str = "qbitai_news.json"):
        """
        ä¿å­˜æ–°é—»åˆ—è¡¨ä¸ºJSONæ–‡ä»¶
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import json
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(news_list, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ–°é—»å·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")


def main():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    crawler = QbitAICrawler()
    
    # æŠ“å–TOP10æ–°é—»
    news_list = crawler.fetch_top_news(limit=10)
    
    # æ‰“å°ç»“æœ
    if news_list:
        print("\n" + "="*70)
        print("ğŸ“‹ æŠ“å–ç»“æœæ±‡æ€»")
        print("="*70 + "\n")
        
        for idx, news in enumerate(news_list, 1):
            print(f"\nã€{idx}ã€‘{news['title']}")
            print(f"   ä½œè€…: {news['author']} | æ—¶é—´: {news['time']}")
            print(f"   æ ‡ç­¾: {', '.join(news['tags']) if news['tags'] else 'æ— '}")
            print(f"   æ‘˜è¦: {news['summary'][:100]}...")
            print(f"   é“¾æ¥: {news['url']}")
        
        # ä¿å­˜ä¸ºJSON
        crawler.save_to_json(news_list, "data/qbitai_top10.json")
    else:
        print("\nâŒ æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»")


if __name__ == "__main__":
    main()
